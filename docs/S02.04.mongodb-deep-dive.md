- [S02.03 MongoDB deep dive](#s0203-mongodb-deep-dive)
  - [SQL vs NoSQL](#sql-vs-nosql)
    - [Workouts example - data is flexible](#workouts-example---data-is-flexible)
    - [Performance Comparison PostgreSQL vs MongoDB](#performance-comparison-postgresql-vs-mongodb)
    - [OLTP vs OLAP](#oltp-vs-olap)
      - [Wrap up statement](#wrap-up-statement)
    - [Index creation and explain](#index-creation-and-explain)
  - [Schema design](#schema-design)
    - [Referencing vs Embedding](#referencing-vs-embedding)
      - [Embedding Advantages](#embedding-advantages)
      - [Embedding Limitations](#embedding-limitations)
      - [Referencing](#referencing)
        - [Referencing Advantages](#referencing-advantages)
        - [Referencing Limitations](#referencing-limitations)
    - [Type of relationships](#type-of-relationships)
      - [One-to-One](#one-to-one)
      - [One-to-Few](#one-to-few)
      - [One-to-Many](#one-to-many)
      - [One-to-Squillions](#one-to-squillions)
      - [Many-to-Many](#many-to-many)
    - [Massive arrays anti pattern](#massive-arrays-anti-pattern)
    - [Sharding vs replication](#sharding-vs-replication)
  - [Schema enforcement and validation](#schema-enforcement-and-validation)
  - [Enforcing a data type](#enforcing-a-data-type)
  - [Conclusion](#conclusion)

# S02.03 MongoDB deep dive

Retrieving data through querying is one important aspect of working with a database.

However if you already know SQL, it's straightforward to give the database schema as background context to an LLM (chatGPT, Claude, ...) and convert the query from SQL to MongoDB. In fact, LLMs are getting better and better at generating queries that answer a given question in natural language.

More important is learning what characterizes MongoDB

- use cases
- schema design
- specific features
- optimization

The goal is that you have a good understanding of

1. when to use SQL or NoSQL
2. how to design a NoSQL database

## SQL vs NoSQL

- use cases see medium article
- performance see other ,medium article

I would say that this is the first question at the early stage of your application design.

You've outgrown a simple sqlite or JSON based data store. what should you aim for next ?

An excellent article on when to use NoSQL

https://medium.com/@sqlinsix/when-to-use-sql-or-nosql-b50d4a52c157

(available as pdf in the github repo)

questions to ask

- is the data structure / schema expected to change often
- is the application read or write intense

- How often will I be reading and writing the data?
- How often will the structure of the data change?
- How much of the data is unknown â€” meaning that more can be added later that I may be unaware of?
  - compare financial transactions (we know all types) and workouts (come in all shapes and sizes) or food
- What does the final product look like?


NoSQL is most appropriately used for document data

- journal that vary in content, length etc
- configuration files where keys may be present or not. not consistent anyway

SQL is most efficient

- for transactional systems ? âœ…
- data schema does not change often ? âœ…
- high writes ? ðŸ¤”

NoSQL :

- document data ? âœ…
- flexible data structure ? âœ…
- high reads ? â“ ðŸ¤”


### Workouts example - data is flexible

Imagine a workout dataset
We have:

- users
- sessions
- gym & locations
- subscriptions
- etc ...

all very standard SQL compliant

and we also have **workouts** that by nature can vary a lot. (see cross-fit)

- weights and reps : dead lifts, clean and jerks, ...
- burpees, Turkish get up, sit-ups, squats, planking, ...
- running, rowing, ...
- bikes, walks, ...
- power yoga, pilates, Zumba, ...

It varies so much that shoe boxing the data into a fixed structure would be difficult.

"The join complexity would be enormous relative to a personâ€™s workout, as there may be dozens of combinations. In addition, workouts such as drop sets where the weight is constantly changing and a maximum (or minimum) repetition range must be met would be extremely challenging to track with a set schema using SQL."

That's were a flexible schema data structure makes sense

NoSQL from the start, Let's think about the data entity itself â€” for an example, we think about the entire workout of day 2017-01-01 as a whole.

We donâ€™t need joins to return all the related data from a normalized data process because we store by the entire entity.

Other good examples:

- a piece of music: vocals, genre, duration, beat, artists, musicians, instruments, software, etc etc etc
- a customerâ€™s mailing address? "It's a terrible idea in a SQL database because the normalization approach to this lacks an understanding of business necessity. Very few people are updating their address on a daily basis, so optimizing for writes makes little sense. An address is going to be read more than written. NoSQL is a much better way to store addresses and address history because (1) weâ€™ll be reading it much more than writing a new address and (2) NoSQL makes it easy to store historical addresses in a subdocument."

What other type of datasets can you think of ?


### Performance Comparison PostgreSQL vs MongoDB

https://medium.com/@vosarat1995/postgres-vs-mongo-performance-comparison-for-semi-structured-data-9a5ec6486cf6

| Test                                | Mongo     | Postgres  |
|-------------------------------------|-----------|-----------|
| Index creation on an empty database | 152.9 ms  | 402.0 ms  |
| Batch insert                        | 53.50 ms  | 219.15 ms |
| One by one insert                   | 319.3 ms  | 641.9 ms  |
| Read multiple records               | 21.83 ms  | 66.63 ms  |
| Index creation on filled database   | 1.563 s   | 1.916 s   |
| Aggregated query                    | 174.51 ms | 60.43 ms  |


### OLTP vs OLAP

2 main database usage types: transactions or dahsboards

**OLTP** (Online Transaction Processing) systems focus on handling a large number of short, atomic transactions in real-time. They are optimized for write operations and transactional integrity, commonly used for applications like e-commerce, banking, and inventory management. In OLTP systems, we optimize for writes as much as possible.
Data integrity and noramlization is key

**OLAP** (Online Analytical Processing) systems are designed for querying and analyzing large datasets, often involving complex joins, aggregations, and data transformations. These systems are read-intensive and require optimized data retrieval. Think dashboards. Denormalization simplifies queries increase performance.

So, should we use NoSQL or SQL for OLAP ? for OLTP ?

https://galaktika-soft.com/blog/sql-vs-nosql.html

"SQL databases are ideal for projects with the following features:

- Logical data requirements can be defined in advance;
- When data integrity is very important;
- A need for well-established technology based on well-established standards, using which one can count on extensive experience of developers and technical support.

You should choose NoSQL databases if in your products:

- Data requirements are vague, uncertain, or evolving with project development;
- The goal of the project can be adjusted over time, with the possibility of starting development immediately;
- One of the main database requirements is data processing speed and scalability.
"


Points to consider:

- Single data point vs large volumes
- aggregation pipelines are slower than complex SQL queries
- reads vs writes
- storage and indexing
- ACID compliance : Mongodb ensures ACID since v4 https://www.mongodb.com/products/capabilities/transactions and see https://www.mongodb.com/resources/products/capabilities/acid-compliance
- MongoDB scales horizontally, SQL vertically

and most important factor

- how is the data consumed by the app

- For OLTP workloads: MongoDB's document model excels at handling many small, quick transactions because each document contains related data in a single unit.

- For OLAP workloads: MongoDB has some limitations that make it less ideal for complex analytical queries:
  - The aggregation pipeline, while powerful, can become quite complex for sophisticated analytical queries that would be simpler in SQL. Joining large amounts of data across collections can be memory-intensive and slower compared to traditional RDDBMs.
  - MongoDB's storage engine is optimized for random access patterns (typical in OLTP) rather than the sequential scans common in analytical workloads.

#### Wrap up statement

This statement sheds a light on when to use NoSQL

from : https://softwareengineering.stackexchange.com/a/355964/440337

>_NOSQL is very much designed as a persistence layer for an application. So access is optimised for a single object with children._

Many things to unfold here :

- persistence layer: the storage of the data
The term "persistence layer" refers to the part of an application architecture responsible for storing and retrieving data in a way that survives beyond the application's runtime. It's where data gets saved permanently, rather than just existing temporarily in memory.

NoSQL databases are built specifically to serve applications' data storage needs. This differs from traditional databases that were often designed first for data organization and relationships, with application needs being secondary.

A "single object" refers to a primary data entity (like a user profile, a product, or a document)

"Children" refers to the related data that belongs to or is closely associated with that primary object (like a user's addresses, a product's reviews, or a document's attachments)

the way NoSQL databases **physically store data**. Instead of splitting related data across multiple tables as in relational databases, NoSQL often stores the primary object and its children together in a single location on disk. This means retrieving all related data requires fewer disk operations.

This statement also implies a key architectural principle: NoSQL databases are designed around the specific ways applications need to access and manipulate data, rather than around maintaining formal data relationships. This makes them particularly effective when an application needs to quickly retrieve or update a complete object and all its associated data in a single operation.

**Single** being the important word here.


The optimization for single objects comes from the physical storage architecture of NoSQL databases. When you store a document in MongoDB, that document and all its nested data are stored contiguously on disk - meaning all the bytes are written next to each other in the physical storage. This makes retrieving that single document very fast, as it requires minimal disk seeks and reads.

However, when dealing with multiple objects or large volumes, several technical factors come into play that affect performance:
First, NoSQL databases typically don't maintain the same type of sophisticated indexes that relational databases do for joining and relating data across different collections. While MongoDB does support indexes, they are primarily designed to help locate individual documents rather than optimize complex queries across multiple documents.
Second, when you need to access multiple objects that aren't physically stored near each other, the database needs to perform multiple random disk seeks. These random seeks are much slower than sequential reads because the disk head needs to physically move to different locations. This is why operations that need to scan many documents (like analytical queries) can be slower in MongoDB compared to databases designed for that purpose.
Third, MongoDB's memory management is optimized around the concept of working sets - the documents that are most frequently accessed. When operating on large volumes of data that exceed the size of RAM, MongoDB needs to constantly swap documents in and out of memory, which can significantly impact performance.

### Index creation and explain

see https://chatgpt.com/c/6750283d-9f4c-800e-99ed-930e0ad24cf3

We won't go into the types of indexes for NoSQL database. but just illustrate the query planning of a few queries with and without indexes

to EXPLAIN a query

- aggregation pipelines: `db.movies.explain("executionStats").aggregate(<the_pipeline>)`
- direct querying: `db.movies.find({query predicate}).explian("executionStats")`

This returns a lot of information.

Look for

- `totalDocsExamined`
- `nReturned`
- `executionTimeMillisEstimate` : estimate the cumulative execution time for the pipeline up to and including that stage. top is highest
- `rejectedPlans`
- `stage`  : COLLSCAN, IXSCAN

compare

db.movies.find({ "title": "The Perils of Pauline" }).explain("executionStats")

Then create an index
db.movies.createIndex({ title: 1 })

db.movies.find({ "title": "The Perils of Pauline" }).explain("executionStats")

Drop the index
db.movies.dropIndex({title : 1})

Explain an aggregation pipeline

db.movies.explain("executionStats").aggregate([ { $group: { _id: "$genres", averageRating: { $avg: "$imdb.rating" } } }] )


## Schema design

MongoDB schema design is the most critical part of deploying a scalable, fast, and affordable database

If you design the MongoDB schema like a relational schema you lose the benefits and power of NoSQL

MongoDB Schema Design: Data Modeling Best Practices

https://www.mongodb.com/developer/products/mongodb/mongodb-schema-design-best-practices/

It depends. This is because document databases have a rich vocabulary that is capable of expressing data relationships in more nuanced ways than SQL. There are many things to consider when picking a schema. Is your app read or write heavy? What data is frequently accessed together? What are your performance considerations? How will your data set grow and scale?


The tl;dr of **normalization** is to split up your data into tables, so you don't duplicate data.

- 1 user
- has many cars
- has many jobs

In sql : 3 tables, apply Normalization forms etc

With MongoDB schema design, there is:

- No formal process
- No algorithms
- No rules

Not Scary at all !!! ðŸ˜³ðŸ˜³ðŸ˜³

What matters is that you design a schema that will work well for your application. Two different apps that use the same exact data might have very different schemas if the applications are used differently.


### Referencing vs Embedding

#### Embedding Advantages

- You can retrieve all relevant information in a single query.
- Avoid implementing joins in application code or using $lookup.
- Update related information as a single atomic operation. By default, all CRUD operations on a single document are ACID compliant.
- However, if you need a transaction across multiple operations, you can use the transaction operator.
- Though transactions are available starting 4.0, however, I should add that it's an anti-pattern to be overly reliant on using them in your application.

#### Embedding Limitations

- Large documents mean more overhead if most fields are not relevant. You can increase query performance by limiting the size of the documents that you are sending over the wire for each query.
- There is a **16-MB document size limit in MongoDB**. If you are embedding too much data inside a single document, you could potentially hit this limit.


#### Referencing

Okay, so the other option for designing our schema is referencing another document using a document's unique object ID and connecting them together using the $lookup operator. Referencing works similarly as the JOIN operator in an SQL query. It allows us to split up data to make more efficient and scalable queries, yet maintain relationships between data.

##### Referencing Advantages

- By splitting up data, you will have smaller documents.
- Less likely to reach 16-MB-per-document limit.
- Infrequently accessed information not needed on every query.
- Reduce the amount of duplication of data. However, it's important to note that data duplication should not be avoided if it results in a better schema.

##### Referencing Limitations

- In order to retrieve all the data in the referenced documents, a minimum of two queries or `$lookup` required to retrieve all the information.

### Type of relationships

Consider the types of relations between entities

#### One-to-One

Modeled as key value pairs in the database. For instance

- movie <-> title
- user <-> last names
- product <-> price

#### One-to-Few

A small sequence of data that's associated with the main entity.

- a person has a few addresses
- a ticket has a few prices
- a playlist has a few tracks
- a recipe has a few ingredients
- a Parisian has a few favorite cafes

> Rule 1: Favor embedding unless there is a compelling reason not to.

```text
Prefer embedding for one-to-few relationships.
```

#### One-to-Many

- a gym has many users
- a product has many parts
- a bus line has many stops
- a TikToker has many followers


Rule 2: Needing to access an object on its own is a compelling reason not to embed it.

Rule 3: Avoid joins/lookups if possible, but don't be afraid if they can provide a better schema design.

#### One-to-Squillions

What if we have a schema where there could be potentially millions of subdocuments, or more? That's when we get to the one-to-squillions schema.


a server logging application. Each server could potentially save a massive amount of data, depending on how verbose you're logging and how long you store server logs for.


 instead of tracking the relationship between the host and the log message in the host document, let's let each log message store the host that its message is associated with. By storing the data in the log, we no longer need to worry about an unbounded array messing with our application! Let's take a look at how this might work.

Rule 4: Arrays should not grow without bound. If there are more than a couple of hundred documents on the "many" side, don't embed them; if there are more than a few thousand documents on the "many" side, don't use an array of ObjectID references. High-cardinality arrays are a compelling reason not to embed.

#### Many-to-Many
The last schema design pattern we are going to be covering in this post is the many-to-many relationship. This is another very common schema pattern that we see all the time in relational and MongoDB schema designs. For this pattern, let's imagine that we are building a to-do application. In our app, a user may have many tasks and a task may have many users assigned to it.

Users:

{

    "_id": ObjectID("AAF1"),

    "name": "Kate Monster",

    "tasks": [ObjectID("ADF9"), ObjectID("AE02"), ObjectID("AE73")]

}

Tasks:

{

    "_id": ObjectID("ADF9"),

    "description": "Write blog post about MongoDB schema design",

    "due_date": ISODate("2014-04-01"),

    "owners": [ObjectID("AAF1"), ObjectID("BB3G")]

}

From this example, you can see that each user has a sub-array of linked tasks, and each task has a sub-array of owners for each item in our to-do app.

Rule 5: As always, with MongoDB, how you model your data depends â€“ entirely â€“ on your particular application's data access patterns. You want to structure your data to match the ways that your application queries and updates it.


Recap:

- One-to-One - Prefer key value pairs within the document
- One-to-Few - Prefer embedding
- One-to-Many - Prefer embedding
- One-to-Squillions - Prefer Referencing
- Many-to-Many - Prefer Referencing

General Rules for MongoDB Schema Design:

- Rule 1: Favor embedding unless there is a compelling reason not to.
- Rule 2: Needing to access an object on its own is a compelling reason not to embed it.
- Rule 3: Avoid joins and lookups if possible, but don't be afraid if they can provide a better schema design.
- Rule 4: Arrays should not grow without bound. If there are more than a couple of hundred documents on the many side, don't embed them; if there are more than a few thousand documents on the many side, don't use an array of ObjectID references. High-cardinality arrays are a compelling reason not to embed.
- Rule 5: As always, with MongoDB, how you model your data depends entirely on your particular application's data access patterns. You want to structure your data to match the ways that your application queries and updates it.

### Massive arrays anti pattern

Sceham design patterns

https://learn.mongodb.com/learn/course/schema-design-patterns/lesson-2-computed-pattern/learn?client=customer



Let's go through this article

https://www.mongodb.com/developer/products/mongodb/schema-design-anti-pattern-massive-arrays/


The outlier pattern

https://www.mongodb.com/blog/post/building-with-patterns-the-outlier-pattern

### Sharding vs replication

> **Sharding** is the process of splitting large datasets across multiple servers (shards) to provide scalability and ensure performance. With sharding, MongoDB can handle large amounts of data and high query loads by distributing the data across a cluster of servers.

see <https://www.mongodb.com/docs/manual/sharding/> and <https://www.mongodb.com/resources/products/capabilities/sharding>

Sharding is useful when:

- The dataset becomes too large for a single database server.
- Query performance degrades due to high read/write loads.
- Scalability is required for handling increased user traffic.

MongoDB has **native sharding capabilities** but sharding is a broad concept applicable to many database systems (often though extensions).

## Schema enforcement and validation

## Enforcing a data type

But it's possible to enforce a data type when declaring the schema

```bash
db.createCollection("movies", {
    validator: {
        $jsonSchema: {
            bsonType: "object",
            required: ["year", "title"],  // Fields that must be present
            properties: {
                year: {
                    bsonType: "int",  // Enforces that `year` must be an integer
                    description: "Must be an integer and is required"
                },
                title: {
                    bsonType: "string",
                    description: "Title of the movie, must be a string"
                },
                imdb: {
                    bsonType: "object",  // Nested object for IMDb data
                    properties: {
                        rating: {
                            bsonType: "double",  // IMDb rating must be a double
                            description: "Must be a double if present"
                        }
                    }
                }
            }
        }
    }
})
```

Key Points:

- bsonType: Specifies the BSON data type for the field (e.g., int, string, array, object).
- required: Ensures specific fields are mandatory.
- properties: Defines the constraints for each field.
- description: Adds a helpful description for validation errors.

MongoDB supports schema validation starting from version 3.6, which allows you to enforce data types and other constraints on fields within a collection. This is achieved using the $jsonSchema validation feature when creating or updating a collection.


## Conclusion

There is no avoiding complexity. The product team will always come up with a new feature or requirement that's going to mess up your carefully designed schema. Whether it's SQL or NoSQL.

Shaving off a few milliseconds will not compensate the costs of more developer hours.

It's not impossible to transfer a MongoDB to a SQL db later on in the project.

MongoDB is cool. PostgreSQL too.

